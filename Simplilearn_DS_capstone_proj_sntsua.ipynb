{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc8e0d32-d570-4f11-be85-7fdd618b2227",
   "metadata": {},
   "source": [
    "\n",
    "*** Data Science Capstone Project Presentation***\n",
    "---\n",
    "### In fulfillment of Simplilearn Master Data Science Certification course\n",
    "\n",
    "## Project_name: Healthcare - NIDDK (National Institute of Diabetes and Digestive and Kidney Diseases)\n",
    "---\n",
    "Presenter_:_***Samuel_Y._Ntsua***\n",
    "\n",
    "### Trainer and Mentor : **TBD**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2feaa81-3146-4ba8-89c8-12e08662a450",
   "metadata": {},
   "source": [
    "** install packages : <br>\n",
    "** remotezip : for query and downloand zip folder from a url <br>\n",
    "** plotly, seaborn :  interactive graphs <br>\n",
    "** cufflinks : connects plotly with pandas to create graphs and charts of dataframes directly <br>\n",
    "** textblob : process textual data<br>\n",
    "** missingno : visualize missing data<br>\n",
    "** scikit-learn : missing data treatment and model evaluation<br>\n",
    "** pandas-profiling: detailed EDA.<br>\n",
    "** In addition tho these, we will be installing other libraries down the road, as and when they are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bccbb77-0a11-4045-a382-cc6661366b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install remotezip cufflinks plotly textblob seaborn missingno  # Comented out because this is needed only once.\n",
    "# %pip install pandas_profiling[notebook]\n",
    "# %conda install -c conda-forge scikit-learn pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1ff02e-f5bc-4262-bf48-c61ce0610fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking content of zip folder so I can determine a strategy to load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be7a6cd-59d7-4a4f-83b9-954f3246e851",
   "metadata": {},
   "outputs": [],
   "source": [
    "%lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3d4e1b-9a7b-42be-a18d-51c4f02b5c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore harmless warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Imports\n",
    "import os, pandas as pd, numpy as np, seaborn as sns, plotly.express as px, cufflinks as cf, matplotlib.pyplot as plt, missingno as missno\n",
    "#os.listdir(\"Project2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85fdd4b-88aa-4078-a503-1bb71bb69124",
   "metadata": {},
   "source": [
    "** Check the content of the ziped folder located on github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a5d4de-ea6e-4aec-b448-0836cf03ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from remotezip import RemoteZip\n",
    "# with RemoteZip('https://github.com/Simplilearn-Edu/Data-Science-Capstone-Projects/raw/master/Project_2.zip') as hczip :\n",
    "#     for hcfiles in hczip.infolist():\n",
    "#         print(hcfiles.filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bb375c-4ae4-488f-99a4-54aaca7f5993",
   "metadata": {},
   "source": [
    "**Now that we see the contents, we can grab the specific file we need for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a44e397-54e8-4d07-b17c-b533905059c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with RemoteZip('https://github.com/Simplilearn-Edu/Data-Science-Capstone-Projects/raw/master/Project_2.zip') as hczip :    \n",
    "#    hczip.extract('Project 2/Healthcare - Diabetes/health care diabetes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec2becf-2cff-4e8b-b7f2-7489b122ee8e",
   "metadata": {},
   "source": [
    "**The file is now downloaded to my local machine at 'Project 2/Healthcare - Diabetes/health care diabetes.csv'<br>\n",
    "** We can now load it with pandas<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9e6adb-03df-4e3a-ab17-b7f03949a9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_df = pd.read_csv('Project 2/Healthcare - Diabetes/health care diabetes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae9b69d-aa56-41d9-bf6a-98742ceda302",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0f18be-a798-43e2-ad8d-c23bcc9d9e0c",
   "metadata": {},
   "source": [
    "** Descriptive analysis and data understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c33d5d6-5ccd-466c-936c-ad2db4a5784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f1423f-b86d-4c92-a0d2-58c658b3c49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(hc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eb0021-d1a5-45ce-8f14-9113baf02b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da3e981-6f83-4c8e-ac85-7151fa35514f",
   "metadata": {},
   "source": [
    "** Visual exploration and checking for missing data<br>\n",
    "The project instruction indicates that a value of 0 in <br>\n",
    "Glucose, BloodPressure, SkinThickness, Insulin, BMI are actually missing values<br>\n",
    "So let's go ahead and set the 0s to np.nan <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5cd6f2-5894-4bad-bd12-931b0dce3e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hc_df.columns = hc_df.columns.map(str.lower) # all column names to lowercase\n",
    "zcol = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "#hc_df=hc_df[hc_df[zcol].astype(float)]\n",
    "#hc_df=hc_df.astype({'Glucose':float, 'BloodPressure':float, 'SkinThickness':float, 'Insulin':float, 'BMI':float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77b6920-7d63-43f4-b666-265d29bf9de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e657cfa-d5cd-4f89-8ccd-4d11b700773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_df[hc_df[zcol]==0]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40497318-18f2-4b7b-a9ee-d9d0dca708c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d03bf3a-a8f6-402d-9cfe-1a4017f9d4ce",
   "metadata": {},
   "source": [
    "** The same information as above, but now in percentge<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ba80dc-7e1e-4a52-b5a9-039ae9efc205",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(100*(hc_df.isnull().sum() / len(hc_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cbdfdf-6b63-4a80-b7c1-4c7956cb0744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %history -g -f cmd_hist.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddae5fd-9d9d-4d9d-90b5-6b1ec7f0dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %lsmagic\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3b7ce4-52d9-4132-a618-713b17310dcb",
   "metadata": {},
   "source": [
    "** To visually see how missing value in one column is related to another column, \n",
    "*** let's plot the heatmap of the missingness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8787ff87-99d9-44fb-acc9-450c10007fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(hc_df.isnull(),cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1da60b-998a-4be4-bd98-b2033afee7e6",
   "metadata": {},
   "source": [
    "** Furthermore, we can use missno's matrix to highlight places in each column where data is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06c89b9-cf5b-43a4-8074-f117bc1cd504",
   "metadata": {},
   "outputs": [],
   "source": [
    "missno.matrix(hc_df,figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60885a4d-e51c-4304-8e89-82fb78e3a95b",
   "metadata": {},
   "source": [
    "** Dropping missing a missing value from a column leads to dropping all other valid <br>\n",
    "** values in the row corresponding to the missing value. <br>\n",
    "** Dropping valid data will then lead to bias in the results.<br>\n",
    "** To avoid such problem, we need to examine how much good data will be thrown out when a targeted missing data is dropped.<br>\n",
    "** A correlation metrix between the missing variables can tell us the how one missing value is related to other non-missing values.<br>\n",
    "** Below, we see the missno heatmap of missingness correlated matrix.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ec1775-07e1-47ae-a8e6-114cf1195306",
   "metadata": {},
   "outputs": [],
   "source": [
    "missno.heatmap(hc_df,figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634f03c4-efb7-47af-bd8f-0131785c81b9",
   "metadata": {},
   "source": [
    "** Likewise, a dendogram also show how one missing value in one column is related to non-missing value in another column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260852b6-7d32-422a-bbcd-91f7056ce0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "missno.dendrogram(hc_df,figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0136d34-9f59-42b6-bb50-bf59398de72d",
   "metadata": {},
   "source": [
    "\n",
    "** From the graphs above we can observe that:<br>\n",
    "** (1) `Insuline` and `SkinThinckness` are correlated (corr coef =0.7)<br>\n",
    "** (2) There are more missing data in `Insulin` and `SkinThickness`, but <br>\n",
    "** each time `Insulin` is missing `SkinThickness` is also missing, however, not the other way round.<br>\n",
    "** In theory, given this correlation, the absence of one of `Insulin` or `SkinThickness` will not affect the ability to predict<br>\n",
    "** the `Outcome` varibale. That is, one of the correlated variable can be expressed in termes of the other.<br> \n",
    "** But if we were to drop one, further analysis is needed to determine the one that is not a `principal component` contributing<br> \n",
    "** variation to `Outcome`. Furthermore, when two variables are correlated, it do not necessarily mean one of then is useless. <br>\n",
    "** In fact ,in this dataset, the measures of `Insulin` or `SkinThickness` were observed naturally as they occured on the patients.<br> \n",
    "** These observations suggest that dropping these missing values can lead to loosing data pont that would otherwise <br>\n",
    "** contribute to the accuracy of our analysis results. <br>\n",
    "** So for now we continue our analysis without dropping any variable.<br> \n",
    "\n",
    "** Going forward, we need to figure out how to handle the missing values then.<br> \n",
    "\n",
    "** Do we fill them with mean values? Probably not. Why?<br> \n",
    "** Although replacing almost half of the values with the mean value is not going the affect the mean of that same variable, <br>\n",
    "** it however reduces it's standard error, and so affecting its relationship with other variables in the dataset.<br> \n",
    "** Doing so will likely tilt the imputed mean towards the observed mean.<br> \n",
    "\n",
    "** Replacing missing values with the mean constitute a quick fix that will get me the project<br>\n",
    "** completed quickly, but it comes with flawed prediction capabilities.<br>\n",
    "\n",
    "** As such, I would not opt for that option.<br>\n",
    "** What else can be done without loosing predictive capability of the data? <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c7dcd4-08d7-4e03-8c79-782086e894db",
   "metadata": {},
   "source": [
    "** Now let's look at two other treatment methods of missing values: <br>\n",
    "** multiple imputation (`mi`) and maximum likelihood estimate (`mle`). <br>\n",
    "** `maximum likelihood estimate` `mle`: <br>\n",
    "** `mle` takes the row on which data is missing, then compare the non-missing values of that row to other non-missing <br>\n",
    "** value in the same column (within variable), then determins the closet of the set of non-missing values (the likelyhood),<br>\n",
    "** and finally look up the corresponding value in the missing value's column to replace the actual missing value. <br>\n",
    "** Another way to view this is, if two subject have the same values of parameter except that one is missing for a subject, <br>\n",
    "** it is logical to replace the missing value with the corresponding parameter of the other subject. <br>\n",
    "** Problem with `mle` treatment of missing value <br>\n",
    "** `mle` does not impute data. <br>\n",
    "** Given the description of the method above, it is clear that the replacement of the missing value is `linear` in nature, <br>\n",
    "** and therefore `mle` applies to linear models only. <br>\n",
    "** `multiple imputation` `mi`: <br>\n",
    "** As the name suggests, `mi` imputes multiple times, that is, it takes multiple and different samples (of same size) <br>\n",
    "** from the original data (nonparametric bootstrap), compute an estimator $\\hat{X_{i}}$ of the missing value from each sample, <br>\n",
    "** then based on the assumption that, the missing value we are trying to figure out follows the same distribution as  $\\hat{X_{i}}$, <br>\n",
    "** we compute an estimate $\\bar{x}$ of the missing value. <br>\n",
    "** Statistical software like Stata, SAS, SPSS and R implement various computation methods of `mi`. <br>\n",
    "** In Python I am going to use scikit-learn implimentation (IterativeImputer), even though it is still experimental as of today. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b6aa76-92d4-44f1-88c4-6c90f2f5bd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a40591-85b5-4501-a1cf-d3da7e93fbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = IterativeImputer(random_state=100)\n",
    "imp.fit(hc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee0bd53-52b9-47ad-9c4a-aaeb6b103ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_hc_df = pd.DataFrame(imp.transform(hc_df), columns=hc_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4985ef8-3d3f-4071-a85b-ca08ddcbfcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(100*(imputed_hc_df.isnull().sum() / len(imputed_hc_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dead2f8c-ef59-4c15-90d8-5b9d63657cfc",
   "metadata": {},
   "source": [
    "### Now that we have full dataset, we can steam forward  with more data exploration.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c31de6-abcf-4f67-b3c3-3b0737866433",
   "metadata": {},
   "source": [
    "** Let's look at the Mean, std, min, max and quantiles of `before imputation`, for both `Outcome` ==0 and `Outcome` ==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b51c329-01f6-41c8-888d-c43aecc523af",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d592e07e-dcad-45ae-9e7c-2f1b78e9cdee",
   "metadata": {},
   "source": [
    "The above output of `mean` and  `max` implies that there are some extremes values in the dataset.<br>\n",
    "For instance, `Pregnancies` has a max value of 17.<br>\n",
    "This suggests someone in the dataset was pregnant 17 times! Is this realistc?<br>\n",
    "`Insulin` also seems to have an extreme case.<br>\n",
    "Let's look at the historgram of the variable to see if there are extreme cases to worry about.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680e4c2a-a281-420e-be2d-d69340dc874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i, c in enumerate(hc_df.columns):\n",
    "    plt.subplot(5,2,i+1)\n",
    "    sns.histplot(hc_df[c])\n",
    "    plt.title('Distribution plot for field:' + c)\n",
    "    plt.xlabel('')\n",
    "    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de122d06-8a44-402c-ac67-6132b6b21a81",
   "metadata": {},
   "source": [
    "Looking at the graph above, two cases of `Pregnancies` and `Insulin` need explanation: <br>\n",
    "`Pregnancies` : Even though it seems unusual for humans to be pregnant 17 times, the the graph shows no gap between<br>\n",
    "the max value of 17 and the rest of the group. This suggest it is not an outlier case.<br><br>\n",
    "`Insulin`: there are a few observations in the same region as the max value of 846.<br>\n",
    "Even though these values are extremely high, the `75 percentile` shows that 75% of the data has value less than 190<br>\n",
    "Furthermore, the `mean` of `155` and `standard deviation` of `118` shows that the bulk of the data are within normal range.<br>\n",
    "Thus, the values of `Insulin` in the region of 800 will not significantly affect the overall statistical validity of that variable.<br>\n",
    "`SkinThickness` and `MBI` seem a little skewed, but not to the level that warrant correction before analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111ac425-2e6d-481b-9ca4-2b9b0e7d2db0",
   "metadata": {},
   "source": [
    "** Mean, std, min, max and quantiles of `before imputation`, for `Outcome==0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a1c6b2-33c1-40c8-9034-e949331ee8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_df[hc_df['Outcome']==0].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543461c8-fb67-4066-aacc-f4aff699cb4a",
   "metadata": {},
   "source": [
    "** Mean, std, min, max and quantiles of `before imputation`, for `Outcome==1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e4646c-f112-43fb-92e8-eebd125db053",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_df[hc_df['Outcome']==1].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e280f42-c4ed-4dc9-82ee-099ba5805685",
   "metadata": {},
   "source": [
    "`Differences` of (Mean, std, min, max and quantiles) `before and after inputation`:<br>\n",
    "* We compute and compare the differences for `Outcome==0` and `Outcome==1` <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1678860-53b9-4ab4-8522-942593bb5c39",
   "metadata": {},
   "source": [
    "** Computation for `Outcome==0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35b86aa-741f-4a62-9659-2625138fbd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(imputed_hc_df[imputed_hc_df['Outcome']==0].describe().T -  hc_df[hc_df['Outcome']==0].describe().T ,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0021bb-208f-4d08-b982-ada93a6794d9",
   "metadata": {},
   "source": [
    "** Comment for `Outcome==0`:<br>\n",
    "* `count`: We successfully augmented the count for `SkinThickness`, `Insulin`, `BloodPressure`, `Glucose` and `MBI` <br>\n",
    "as shown in the first column of the table above<br>\n",
    "* `mean`,`std`, `min`, `max` and `quantiles`: Given the magnitude of range (`max` - `min`) shown in the `before inputation`, <br>\n",
    "`744-14 for Insulin, for instance`, it appears the before/after difference for `mean`, `std`, `min`, `max` and `quantiles` <br>\n",
    "are quite small.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ea24e9-b087-4235-b8a3-c2cc6e808c1d",
   "metadata": {},
   "source": [
    "** Computation for `Outcome==1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7a4a96-436f-4f0b-8fec-c617e1672d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(imputed_hc_df[imputed_hc_df['Outcome']==1].describe().T -  hc_df[hc_df['Outcome']==1].describe().T ,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734ae58c-a9d4-4657-9f91-69b8342a535f",
   "metadata": {},
   "source": [
    "** Comment for `Outcome==1`:<br>\n",
    "* `count`: We successfully augmented the count for `SkinThickness`, `Insulin`, `BloodPressure`, `Glucose` and `MBI` <br>\n",
    "as shown in the first column of the table above<br>\n",
    "* `mean`,`std`, `min`, `max` and `quantiles`: Given the magnitude of range (`max` - `min`) shown in the `before inputation`, <br>\n",
    "`846-14 for Insulin, for instance`, it appears the before/after difference for `mean`, `std`, `min`, `max` and `quantiles` <br>\n",
    "are quite small.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48fcc7e-d451-44c4-94ee-22cb54426e5b",
   "metadata": {},
   "source": [
    "In conclusion, we can say the imputation has successfully improved the data without loosing any case.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8562210-c811-48a1-afa5-ab12857bafd7",
   "metadata": {},
   "source": [
    "** But that does not mean that a thoughfully imputed dataset garantees a representative analysis result.<br>\n",
    "** If there are significantly more of a particular outcome that others, the analysis results will be skeweb<br>\n",
    "** if the dataset is not balance or a probability weight is not introduced. <br>\n",
    "** Let's see how balanced is the data after imputation.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76713bb3-c296-42c5-bc41-478037362f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Count for Outcome types : \\n\", imputed_hc_df['Outcome'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c966f1d8-8e78-4a00-97c4-06a363ae5bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(imputed_hc_df['Outcome'])\n",
    "plt.xlabel('Outcome types')\n",
    "plt.ylabel('Count of Outcome types')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda239b3-67f1-412d-816e-fd25d9f105f9",
   "metadata": {},
   "source": [
    "The graph above show that about `2/3` of `Outcome` is `0` or `negative outcome`.<br>\n",
    "This implies that if we were to predict a 0 (negative outcome), we would have achieve an accuracy of 75% with the imbalanced data.<br>\n",
    "\n",
    "The data is imbalanced. Data imbalance can be addressed during (a) the analysis and interpretation <br>\n",
    "of results, including resampling methods or (b) at the model performance and evaluation metrics level, including chaging the metric and penalizing the algorithm computing the metric.<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15905d75-7177-4d09-b538-ef959e0eb685",
   "metadata": {},
   "source": [
    "# For this project, we are going to utilize the resampling method to balance the dataset.\n",
    "# ! pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad9aefc-9d18-406d-98fd-c7ea3729cea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "outcome_maj = imputed_hc_df[imputed_hc_df.Outcome==0]\n",
    "outcome_min = imputed_hc_df[imputed_hc_df.Outcome==1]\n",
    "upsample_outcome_min = resample(outcome_min, replace =True, n_samples = outcome_maj.shape[0], random_state= 9876)\n",
    "#We can now put the balanced and imputed dataset together\n",
    "bal_imp_hc_df = pd.concat([outcome_maj, upsample_outcome_min])\n",
    "# Count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaed33a-181d-44ba-a17a-042925680560",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Count of the values of Outcome :\\n',bal_imp_hc_df.Outcome.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c24efc-66d1-4efe-ac80-232957f9d177",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(bal_imp_hc_df.Outcome)\n",
    "plt.xlabel('Outcome types')\n",
    "plt.ylabel('Count of Outcome types')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acc991c-b403-422d-9e9e-6c8f458f37ee",
   "metadata": {},
   "source": [
    "After oversampling the minority class, we now have `Outcome` values to be 50/50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c894f7-f217-400f-be49-e7697eb7ab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The shape of the data after oversampling \\n {}'.format(bal_imp_hc_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd9bbe5-e7aa-4575-83c3-a4a6ba47261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_imp_hc_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13d357c-2fbc-44a1-b9e3-e564b026b52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(bal_imp_hc_df, hue='Outcome', diag_kind='kde', kind = 'reg')\n",
    "plt.title('Pair scatter plots between variables')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef2a658-e0cc-443c-9f5a-35d907ef1152",
   "metadata": {},
   "source": [
    "Frem the pairplot above, it appears there is a slight correlation between : <br>\n",
    "a-) `Glucose` and `Insulin` <br>\n",
    "b-) `SkinThickness` and `BMI` <br>\n",
    "\n",
    "Let's graph the correlation matrix too to confirm the above observation.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e68c495-2351-44e5-96f1-0e450cdc48e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(dpi=120)\n",
    "sns.heatmap(bal_imp_hc_df.corr(), annot=True,cmap='Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a63c08-72cf-482c-a693-ae330ae8b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_imp_hc_df.corr()['Outcome']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f36acac-df82-4033-a4e0-c88df74aa7c5",
   "metadata": {},
   "source": [
    "As expected, the correlation matrix show the highest correlated pairs are : <br>\n",
    "`Glucose` and `Insulin`  0.68 <br>\n",
    "`SkinThickness` and `BMI` 0.7 <br>\n",
    "The lest correlated pair are `Age` and `DiabetesPedigreeFunction` 0.0057.<br>\n",
    "\n",
    "Taking the factors individually, the most correlated to `Outcome` is `Glucose` (no surprise there) <br>\n",
    "followed by `Insulin`, then `BMI`, then `Age`, then `Pregnancies` and `Skinthickness` and <br>\n",
    "the least correlated with `Outcome` is `DiabetesPedigreeFunction`.<br>\n",
    "This ranking also gives us a rought indication of the princial components that, together, would bring the most variations <br>\n",
    "to `Outcome`, thus the best prediction power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509f3560-b74f-4140-8d50-34b6d17fcb62",
   "metadata": {},
   "source": [
    "## Data Modeling\n",
    "Our data has labels, that is, `Outcome` variable.<br>\n",
    "We can therefore use a supervised learning algorithms to try to understand the relationship between: <br>\n",
    "(a) 'Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age' on one side and <br>\n",
    "(b) `Outcome` on the other side. <br>\n",
    "Given the variables in `(a)`, we are trying to determine if a patient has diabetes `(Outcome ==1)` or not `(Outcome==0)`<br>\n",
    "This is a classification problem.<br>\n",
    "Now, which of the classification algorithms will give us the best prediction model?<br>\n",
    "The figure below from scikit-lean website shows us the path decide.<br>\n",
    "\n",
    "![Choosing the right ML estimator](https://scikit-learn.org/stable/_static/ml_map.png)\n",
    "\n",
    "To figure out which classification model will yield the best prediction of `Outcome`,<br>\n",
    "we can train test a few classification model then evaluate the performance then choose the highest performing model.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f202011-5d60-4e42-9272-6f0df1106dd7",
   "metadata": {},
   "source": [
    "# Strategies for model building and selection.<br>\n",
    "We will attempt several models to see their performance using Area Under the Curve score (AUC).<br>\n",
    "We will then choose the best performing model (or combination of model).<br>\n",
    "<br>\n",
    "** Following scikit-learn's diagram above, I will model the data with the following 5 `estimators` :<br>\n",
    "1) Support Vector machines (SVM) <br>\n",
    "2) KNeighbors Classifier <br>\n",
    "3) three (3) Ensemble Classifiers (RandomForestClassifier, ExtraTreesClassifier and XGBoost)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8996e2-0490-4e50-8724-79f5305843d3",
   "metadata": {},
   "source": [
    " In the first round of model selection, I will look at the `performance report card`  of the `estimators`:<br>\n",
    " `precision`, `recall` and `f1-score`.\n",
    " The top 3 estimators will then be further evaluated by `parameter tuning` for their individual best performance on the data <br>\n",
    " Then they will be ranked by their `AUC score`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b78be5-adab-4805-b77c-c578c4080afe",
   "metadata": {},
   "source": [
    "** Split,Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e65c57-3f9d-43ad-a14a-12b9940d3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model algorights to test on data\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "import xgboost as xgb # !pip install xgboost (if not already installed)\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Methods for model Selection\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score,GridSearchCV\n",
    "# Methods for model performance evaluation\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616fed03-c17a-42fd-aab1-b980ceb95ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = bal_imp_hc_df.drop('Outcome',axis=1), bal_imp_hc_df['Outcome']\n",
    "X_train, X_test, y_train, y_test = train_test_split( X , y, test_size=0.3, random_state=9876)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a20191-fda9-4b6c-bcf5-eac8071e99f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape of the training and testing splits: \\n X_train ==> {X_train.shape} \\n X_test ==> {X_test.shape} \\n y_train ==> {y_train.shape}  \\n y_test ==> {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a2777e-6489-4d2e-8cf9-6725baa93a0c",
   "metadata": {},
   "source": [
    "## Performance report card:Precision, recall  and f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4286b2-a245-4504-8e68-6bdfab919e89",
   "metadata": {},
   "source": [
    "##### Performance report card:  Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b8239-5490-4418-a020-915794cc6b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "y_train_hat = model.predict(X_train)\n",
    "y_test_hat = model.predict(X_test)\n",
    "\n",
    "print(model)\n",
    "print(f'Train performance \\n ====================================================' )\n",
    "print(classification_report(y_train, y_train_hat))\n",
    "\n",
    "print(f'Test performance \\n ====================================================' )\n",
    "print(classification_report(y_test, y_test_hat))\n",
    "\n",
    "print(f'Roc_auc score \\n ====================================================' )\n",
    "print(roc_auc_score(y_test, y_test_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab16fc4-a262-4c8d-bbc8-ebf884f96292",
   "metadata": {},
   "source": [
    "#####  Performance report card: K-Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d490f41-198a-47c0-a63d-2fb3288a8425",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_train_hat = model.predict(X_train)\n",
    "y_test_hat = model.predict(X_test)\n",
    "\n",
    "print(model)\n",
    "print(f'Train performance \\n ====================================================' )\n",
    "print(classification_report(y_train, y_train_hat))\n",
    "\n",
    "print(f'Test performance \\n ====================================================' )\n",
    "print(classification_report(y_test, y_test_hat))\n",
    "\n",
    "print(f'Roc_auc score \\n ====================================================' )\n",
    "print(roc_auc_score(y_test, y_test_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e441b17e-7da4-4363-8f4c-2ce3291cc4a8",
   "metadata": {},
   "source": [
    "##### Performance report card: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e563dbb-7f8a-476b-9acb-6daff75eaa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_jobs=-1,random_state=9876)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_hat = model.predict(X_train)\n",
    "y_test_hat = model.predict(X_test)\n",
    "print(model)\n",
    "print(f'Train performance \\n ====================================================' )\n",
    "print(classification_report(y_train, y_train_hat))\n",
    "\n",
    "print(f'Test performance \\n ====================================================' )\n",
    "print(classification_report(y_test, y_test_hat))\n",
    "\n",
    "print(f'Roc_auc score \\n ====================================================' )\n",
    "print(roc_auc_score(y_test, y_test_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40a4065-a059-4c78-a6c2-aa4d6099128b",
   "metadata": {},
   "source": [
    "##### Performance report card: Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e661bf-4634-408e-90e9-b43f3f63aeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier(random_state=9876)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_hat = model.predict(X_train)\n",
    "y_test_hat = model.predict(X_test)\n",
    "print(model)\n",
    "print(f'Train performance \\n ====================================================' )\n",
    "print(classification_report(y_train, y_train_hat))\n",
    "\n",
    "print(f'Test performance \\n ====================================================' )\n",
    "print(classification_report(y_test, y_test_hat))\n",
    "\n",
    "print(f'Roc_auc score \\n ====================================================' )\n",
    "print(roc_auc_score(y_test, y_test_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b8705e-5fcc-4bb2-99ce-9aa02bc86883",
   "metadata": {},
   "source": [
    "##### Performance report card: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2ce16b-931c-4298-8b9f-eca48566a684",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(random_state=9876)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_hat = model.predict(X_train)\n",
    "y_test_hat = model.predict(X_test)\n",
    "\n",
    "print(model)\n",
    "print(f'Train performance \\n ====================================================' )\n",
    "print(classification_report(y_train, y_train_hat))\n",
    "\n",
    "print(f'Test performance \\n ====================================================' )\n",
    "print(classification_report(y_test, y_test_hat))\n",
    "\n",
    "print(f'Roc_auc score \\n ====================================================' )\n",
    "print(roc_auc_score(y_test, y_test_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b160f3b-6ebf-4bae-8f31-5575ebe6c1ed",
   "metadata": {},
   "source": [
    "# Parameter tuning: Model optimization.\n",
    "Out of the 5 algorithms tested above, I will retain the best 3 AUC score.<br>\n",
    "Random forest, XGBoost and ExtraTrees<br>\n",
    "I will tune their parameters to increase AUC score, by uing cross-validation method `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd5312d-38e6-487d-b113-2da70efdf9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1af1b20-069a-40e2-a971-700b89c0f291",
   "metadata": {},
   "source": [
    "##### Parameter tuning: Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9d1978-f292-4679-a763-b4213852af2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': [100, 200, 400],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_split': [1,2,4,5],\n",
    "    'min_samples_leaf': [1,2,4,5],\n",
    "    'max_leaf_nodes': [4,10,20,50,None]\n",
    "}\n",
    "\n",
    "rf_gs = GridSearchCV(RandomForestClassifier(n_jobs=-1,random_state=9876), params, n_jobs=-1, cv=KFold(n_splits=3), scoring='roc_auc')\n",
    "rf_gs.fit(X_train, y_train)\n",
    "y_pred = rf_gs.predict(X_test)\n",
    "print(f'Best score for Random Forest:, {rf_gs.best_score_} \\n Best parameters found for Random Forest: {rf_gs.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ccf2a8-62df-4fc9-a201-718ed693bb40",
   "metadata": {},
   "source": [
    "##### AUC score: Randon Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940cd015-45ef-484b-b5e6-2388039833f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "rf_roc_auc = roc_auc_score(y_test, rf_gs.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, rf_gs.predict_proba(X_test)[:,1]) \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Random Forest (area = %0.2f)' % rf_roc_auc) \n",
    "plt.plot([0, 1], [0, 1],'--')\n",
    "plt.xlim([0.0, 1.05])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Random Forest ROC curve with best parameters') \n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('RF_ROC')\n",
    "print('Area Under Curve: %.3f' % rf_roc_auc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf61c5f-5ed5-4471-8f03-e7eadba46a79",
   "metadata": {},
   "source": [
    "##### Parameter tuning: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a734f486-e254-477c-b862-27211fdff4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': [100, 200, 400],\n",
    "    'learning_rate': [0.01,0.05,0.1],\n",
    "    'booster': ['gbtree', 'gblinear'],\n",
    "    'gamma': [0, 0.5, 1],\n",
    "    'reg_alpha': [0, 0.5, 1],\n",
    "    'base_score': [0.2, 0.5, 1]\n",
    "}\n",
    "\n",
    "xgb_gs = GridSearchCV(XGBClassifier(n_jobs=-1, random_state=9876), params, n_jobs=-1, cv=KFold(n_splits=3), scoring='roc_auc')\n",
    "xgb_gs.fit(X_train, y_train)\n",
    "print(f'Best score for XGBoost:, {xgb_gs.best_score_} \\n Best parameters found for XGBoost: {xgb_gs.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300ddef7-f984-4fde-a9ae-0a5813a7b5a5",
   "metadata": {},
   "source": [
    "##### AUC score: XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36639cf8-ea42-4163-8893-bb85958415cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_roc_auc = roc_auc_score(y_test, xgb_gs.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, xgb_gs.predict_proba(X_test)[:,1]) \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='XGBoost (area = %0.2f)' % xgb_roc_auc) \n",
    "plt.plot([0, 1], [0, 1],'--')\n",
    "plt.xlim([0.0, 1.05])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('XGBoost ROC curve with best parameters') \n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('XGB_ROC')\n",
    "print('Area Under Curve: %.3f' % xgb_roc_auc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd0e59a-b23f-433c-b663-a58d570384b3",
   "metadata": {},
   "source": [
    "##### Parameter tuning: Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c3c6da-66da-4b28-a053-89622a7fdfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': [100, 200, 400],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_split': [1,2,4,6],\n",
    "    'min_samples_leaf': [1,2,4,6],\n",
    "    'max_leaf_nodes': [4,10,20,40,None]\n",
    "}\n",
    "\n",
    "et_gs = GridSearchCV(ExtraTreesClassifier(n_jobs=-1, random_state=9876), params, n_jobs=-1, cv=KFold(n_splits=3), scoring='roc_auc')\n",
    "et_gs.fit(X_train, y_train)\n",
    "print(f'Best score for ExtraTrees:, {et_gs.best_score_} \\n Best parameters found for ExtraTrees: {et_gs.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950d0d12-52b5-4751-8706-8691810c2f3c",
   "metadata": {},
   "source": [
    "##### AUC score: Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659322b3-5c2f-4307-8548-1e0a24d8169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_roc_auc = roc_auc_score(y_test, et_gs.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, et_gs.predict_proba(X_test)[:,1]) \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Extra Trees (area = %0.2f)' % et_roc_auc) \n",
    "plt.plot([0, 1], [0, 1],'--')\n",
    "plt.xlim([0.0, 1.05])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Extra Trees ROC curve with best parameters') \n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ET_ROC')\n",
    "print('Area Under Curve: %.3f' % et_roc_auc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ea37c7-43f7-46ce-a883-6279e9035c62",
   "metadata": {},
   "source": [
    "# Conclusion: Model selection<br>\n",
    "#### We can see that Extra Trees has the best ability to predict with this dataset.<br>\n",
    "#### With a AUC score of 86%, Extra Trees perfomed better than Random Forest 85%, then XGBoost with 82%.<br>\n",
    "#### KNN didn't even make ti to the top 3 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84b0e14-18c5-4f37-9b0c-7ef7b2be7c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
